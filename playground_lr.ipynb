{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-07T11:33:53.534349100Z",
     "start_time": "2024-01-07T11:33:42.099741900Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_tf import *\n",
    "from parameters import *\n",
    "import socket\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START WORKING ON  0\n",
      "comb 1 / 264\n",
      "/data/gpfs/projects/punim2039/EightK/res/temp/vec_pred/7e4a5c808328461a05da2c8579131aa7acc51513b2e6791a0d376f97dcdf10be/OPT_125m/NEWS_SINGLE/\n",
      "save_name 2012.p already_processed []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "par = get_main_experiments(0, train_gpu=1)\n",
    "par.enc.opt_model_type = OptModelType.OPT_125m\n",
    "par.enc.news_source = NewsSource.NEWS_SINGLE\n",
    "par.train.use_tf_models = True\n",
    "par.train.batch_size = 32\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "temp_save_dir = par.get_res_dir()\n",
    "print(temp_save_dir, flush=True)\n",
    "already_processed = os.listdir(temp_save_dir)\n",
    "save_name = f'{par.grid.year_id}.p'\n",
    "print('save_name', save_name, \"already_processed\", already_processed)\n",
    "if save_name in already_processed:\n",
    "    print(f'Already processed {save_name}', flush=True)\n",
    "else:\n",
    "    trainer = PipelineTrainer(par)\n",
    "    trainer.def_create_the_datasets(filter_func=lambda x: 'mean' in x.split('/')[-1].split('_'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T11:34:28.141439Z",
     "start_time": "2024-01-07T11:33:53.528478300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset element_spec=(TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T11:34:28.162441800Z",
     "start_time": "2024-01-07T11:34:28.144444600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def parse_function(feature, label):\n",
    "    return feature.numpy(), label.numpy()\n",
    "\n",
    "def extract_features_and_labels_np(dataset):\n",
    "    # Use parallel processing to map the parse function\n",
    "    dataset = dataset.map(lambda x, y: tf.py_function(parse_function, [x, y], [tf.float32, tf.int32]),\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Iterate over batches and concatenate results\n",
    "    features, labels = zip(*list(dataset))\n",
    "    return np.concatenate(features), np.concatenate(labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T11:34:28.202440Z",
     "start_time": "2024-01-07T11:34:28.159473600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split data\n",
    "X_train, y_train = extract_features_and_labels_np(trainer.train_dataset)\n",
    "X_test, y_test = extract_features_and_labels_np(trainer.test_dataset)\n",
    "X_val, y_val = extract_features_and_labels_np(trainer.val_dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T11:57:43.183156200Z",
     "start_time": "2024-01-07T11:34:28.175469800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:01:14.947877500Z",
     "start_time": "2024-01-07T11:57:43.184135300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient 1e-05: Validation Accuracy = 0.5229772779273809\n"
     ]
    }
   ],
   "source": [
    "ridge_coefficients = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "best_score = 0\n",
    "best_coefficient = None\n",
    "\n",
    "# Tune the ridge penalty coefficient\n",
    "for C in ridge_coefficients:\n",
    "    model = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, C=C, solver='saga', max_iter=50, n_jobs=-1, verbose=1)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    score = accuracy_score(y_val, model.predict(X_val_scaled))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_coefficient = C\n",
    "\n",
    "    print(f\"Coefficient {C}: Validation Accuracy = {score}\")\n",
    "\n",
    "print(f\"Best Coefficient: {best_coefficient}\")\n",
    "\n",
    "# Combine the training and validation sets\n",
    "X_combined = np.concatenate([X_train, X_val])\n",
    "y_combined = np.concatenate([y_train, y_val])\n",
    "\n",
    "final_scaler = StandardScaler()\n",
    "X_combined_scaled = final_scaler.fit_transform(X_combined)\n",
    "X_test_scaled = final_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Train the final model on the combined dataset\n",
    "final_model = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, C=best_coefficient, solver='saga', max_iter=50, n_jobs=-1, verbose=1)\n",
    "final_model.fit(X_combined_scaled, y_combined)\n",
    "\n",
    "final_model_pred = final_model.predict(X_test_scaled)\n",
    "final_model_pred_prb = final_model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, final_model_pred))\n",
    "print(confusion_matrix(y_test, final_model_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-07T08:01:55.135102400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1233 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 20.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.19      0.27    460751\n",
      "           1       0.53      0.84      0.65    505138\n",
      "\n",
      "    accuracy                           0.53    965889\n",
      "   macro avg       0.52      0.51      0.46    965889\n",
      "weighted avg       0.52      0.53      0.47    965889\n",
      "\n",
      "[[ 85426 375325]\n",
      " [ 81466 423672]]\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, C=1e-3, solver='saga', max_iter=50, n_jobs=-1, verbose=1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:21:51.298941800Z",
     "start_time": "2024-01-07T12:01:15.001882700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Get the probability estimates for each class\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m probabilities \u001B[38;5;241m=\u001B[39m \u001B[43mfinal_model\u001B[49m\u001B[38;5;241m.\u001B[39mpredict_proba(X_test_scaled)\n\u001B[0;32m      4\u001B[0m ids \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([id_batch\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _, _, id_batch, _, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_dataset_with_id],\n\u001B[0;32m      5\u001B[0m                              axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      6\u001B[0m tickers \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(\n\u001B[0;32m      7\u001B[0m     [ticker_batch\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _, _, _, _, ticker_batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_dataset_with_id], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'final_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the probability estimates for each class\n",
    "probabilities = final_model.predict_proba(X_test_scaled)\n",
    "\n",
    "ids = np.concatenate([id_batch.numpy().astype(str) for _, _, id_batch, _, _ in self.test_dataset_with_id],\n",
    "                             axis=0)\n",
    "tickers = np.concatenate(\n",
    "    [ticker_batch.numpy().astype(int) for _, _, _, _, ticker_batch in self.test_dataset_with_id], axis=0)\n",
    "dates = np.concatenate([date_batch.numpy().astype(str) for _, _, _, date_batch, _ in self.test_dataset_with_id],\n",
    "                       axis=0)\n",
    "results_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'date': dates,\n",
    "    'ticker': tickers,\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred,\n",
    "    'y_pred_prb': probabilities[:, 1]\n",
    "})\n",
    "results_df['accuracy'] = results_df['y_pred'] == results_df['y_true']\n",
    "results_df\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:21:53.409941900Z",
     "start_time": "2024-01-07T12:21:51.302941200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 2012\n",
      "2010 2011\n",
      "2004 2009\n",
      "trainer.par.grid.year_id 2012\n",
      "trainer.par.train.T_val 2\n",
      "trainer.par.train.T_train 8\n"
     ]
    }
   ],
   "source": [
    "filter_func=lambda x: 'mean' in x.split('/')[-1].split('_')\n",
    "path_with_records = trainer.par.get_training_dir()\n",
    "end_val = trainer.par.grid.year_id - 1\n",
    "start_val = end_val - trainer.par.train.T_val + 1\n",
    "start_train = trainer.par.grid.year_id - trainer.par.train.T_train\n",
    "end_train = start_val - 1\n",
    "start_test = trainer.par.grid.year_id\n",
    "end_test = trainer.par.grid.year_id - 1 + trainer.par.train.testing_window\n",
    "# tfrecord_files = [os.path.join(path_with_records, f) for f in os.listdir(path_with_records) if '.tfrecord' in f]\n",
    "# tfrecord_files = list(filter(filter_func, tfrecord_files))\n",
    "# trainer.train_dataset = trainer.load_dataset('train',tfrecord_files, trainer.par.train.batch_size, start_train, end_train, shuffle=True)\n",
    "# trainer.val_dataset = trainer.load_dataset('val',tfrecord_files, trainer.par.train.batch_size, start_val, end_val)\n",
    "# trainer.test_dataset = trainer.load_dataset('test',tfrecord_files, trainer.par.train.batch_size, start_test, end_test)\n",
    "# trainer.test_dataset_with_id = trainer.load_dataset('test',tfrecord_files, trainer.par.train.batch_size, start_test, end_test, return_id_too=True)\n",
    "# sample_batch = next(iter(trainer.train_dataset.take(1)))\n",
    "\n",
    "\n",
    "print(start_test, end_test)\n",
    "print(start_val, end_val)\n",
    "print(start_train, end_train)\n",
    "print(\"trainer.par.grid.year_id\", trainer.par.grid.year_id)\n",
    "print(\"trainer.par.train.T_val\", trainer.par.train.T_val)\n",
    "print(\"trainer.par.train.T_train\", trainer.par.train.T_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T10:12:08.851915600Z",
     "start_time": "2024-01-05T10:12:08.843296600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: (32, 768)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mpar\u001B[38;5;241m.\u001B[39mtrain\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;241m==\u001B[39m Normalisation\u001B[38;5;241m.\u001B[39mZSCORE:\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Create a Normalization layer\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     normalization_layer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mNormalization(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 20\u001B[0m     \u001B[43mnormalization_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m     layers\u001B[38;5;241m.\u001B[39mappend(normalization_layer)\n\u001B[0;32m     23\u001B[0m layers\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m     24\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m, input_shape\u001B[38;5;241m=\u001B[39m(trainer\u001B[38;5;241m.\u001B[39minput_dim,), kernel_regularizer\u001B[38;5;241m=\u001B[39mreg_to_use)\n\u001B[0;32m     25\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py:286\u001B[0m, in \u001B[0;36mNormalization.adapt\u001B[1;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madapt\u001B[39m(\u001B[38;5;28mself\u001B[39m, data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    241\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Computes the mean and variance of values in a dataset.\u001B[39;00m\n\u001B[0;32m    242\u001B[0m \n\u001B[0;32m    243\u001B[0m \u001B[38;5;124;03m    Calling `adapt()` on a `Normalization` layer is an alternative to\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;124;03m          argument is not supported with array inputs.\u001B[39;00m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 286\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:258\u001B[0m, in \u001B[0;36mPreprocessingLayer.adapt\u001B[1;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39msteps():\n\u001B[1;32m--> 258\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_adapt_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m    260\u001B[0m             context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "reg_value = trainer.par.train.shrinkage_list[0]\n",
    "reg = tf.keras.regularizers.l1_l2(reg_value, reg_value)\n",
    "\n",
    "tr_data=trainer.train_dataset\n",
    "val_data=trainer.val_dataset\n",
    "reg_to_use=reg\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=trainer.par.train.monitor_metric, patience=trainer.par.train.patience, restore_best_weights=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=trainer.par.train.adam_rate)  # Using AMSGrad variant\n",
    "\n",
    "layers = [\n",
    "    tf.keras.layers.Input(shape=(trainer.input_dim,))\n",
    "]\n",
    "for features, _ in tr_data.take(1):\n",
    "    print(\"Shape of features:\", features.shape)  # Debugging statement\n",
    "\n",
    "# if trainer.par.train.norm == Normalisation.ZSCORE:\n",
    "#     # Create a Normalization layer\n",
    "#     normalization_layer = tf.keras.layers.Normalization(axis=-1)\n",
    "#     normalization_layer.adapt(tr_data.map(lambda x, y: x))\n",
    "#     layers.append(normalization_layer)\n",
    "\n",
    "layers.append(\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(trainer.input_dim,), kernel_regularizer=reg_to_use)\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential(layers)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T09:27:17.073681400Z",
     "start_time": "2024-01-05T09:18:28.570299900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<MapDataset element_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name=None)>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.map(lambda x, y: x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T09:56:20.048137200Z",
     "start_time": "2024-01-01T09:56:20.033187200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (32, 768)\n"
     ]
    }
   ],
   "source": [
    "for features, _ in tr_data.take(1):\n",
    "    print(\"Batch shape:\", features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T10:00:57.982254900Z",
     "start_time": "2024-01-01T10:00:57.178563300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.001, 0.01, 0.1, 1, 10]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.par.train.shrinkage_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T12:11:03.310798100Z",
     "start_time": "2024-01-01T12:11:03.280663400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 122, in adapt_step  *\n        self._adapt_maybe_build(data)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 295, in _adapt_maybe_build  **\n        self.build(data_shape)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py\", line 187, in build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None, None], with unknown axis at index: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m normalization_layer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mNormalization(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mnormalization_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py:286\u001B[0m, in \u001B[0;36mNormalization.adapt\u001B[1;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madapt\u001B[39m(\u001B[38;5;28mself\u001B[39m, data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    241\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Computes the mean and variance of values in a dataset.\u001B[39;00m\n\u001B[0;32m    242\u001B[0m \n\u001B[0;32m    243\u001B[0m \u001B[38;5;124;03m    Calling `adapt()` on a `Normalization` layer is an alternative to\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;124;03m          argument is not supported with array inputs.\u001B[39;00m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 286\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:258\u001B[0m, in \u001B[0;36mPreprocessingLayer.adapt\u001B[1;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39msteps():\n\u001B[1;32m--> 258\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_adapt_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m    260\u001B[0m             context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file0v2b4q0_.py:10\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__adapt_step\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ag__\u001B[38;5;241m.\u001B[39mFunctionScope(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madapt_step\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfscope\u001B[39m\u001B[38;5;124m'\u001B[39m, ag__\u001B[38;5;241m.\u001B[39mConversionOptions(recursive\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, user_requested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, optional_features\u001B[38;5;241m=\u001B[39m(), internal_convert_user_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)) \u001B[38;5;28;01mas\u001B[39;00m fscope:\n\u001B[0;32m      9\u001B[0m     data \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mnext\u001B[39m), (ag__\u001B[38;5;241m.\u001B[39mld(iterator),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m---> 10\u001B[0m     \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_adapt_maybe_build\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mupdate_state, (ag__\u001B[38;5;241m.\u001B[39mld(data),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:295\u001B[0m, in \u001B[0;36mPreprocessingLayer._adapt_maybe_build\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_input_shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;66;03m# Set the number of dimensions.\u001B[39;00m\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_input_shape \u001B[38;5;241m=\u001B[39m data_shape_nones\n\u001B[1;32m--> 295\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py:187\u001B[0m, in \u001B[0;36mNormalization.build\u001B[1;34m(self, input_shape)\u001B[0m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_keep_axis:\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m input_shape[d] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 187\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    188\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll `axis` values to be kept must have known shape. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    189\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot axis: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    190\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput shape: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, with unknown axis at index: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    191\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis, input_shape, d\n\u001B[0;32m    192\u001B[0m             )\n\u001B[0;32m    193\u001B[0m         )\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# Axes to be reduced.\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reduce_axis \u001B[38;5;241m=\u001B[39m [d \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(ndim) \u001B[38;5;28;01mif\u001B[39;00m d \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_keep_axis]\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 122, in adapt_step  *\n        self._adapt_maybe_build(data)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 295, in _adapt_maybe_build  **\n        self.build(data_shape)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\layers\\preprocessing\\normalization.py\", line 187, in build\n        raise ValueError(\n\n    ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None, None], with unknown axis at index: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normalization_layer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalization_layer.adapt(tr_data.map(lambda x, y: x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T09:56:00.549984300Z",
     "start_time": "2024-01-01T09:56:00.436832800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_3/dense_3/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_3/dense_3/embedding_lookup_sparse/Reshape:0\", shape=(None, 1), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_3/dense_3/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 695, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 958, in _create_all_weights\n        self._create_hypers()\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1126, in _create_hypers\n        self._hyper[name] = self.add_weight(\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1352, in add_weight\n        variable = self._add_variable_with_custom_getter(\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_layer_utils.py\", line 121, in make_variable\n        init_val = functools.partial(initializer, shape, dtype=dtype)\n\n    TypeError: the first argument must be callable\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stop\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filef0g2p4fv.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: in user code:\n\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 695, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 958, in _create_all_weights\n        self._create_hypers()\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1126, in _create_hypers\n        self._hyper[name] = self.add_weight(\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1352, in add_weight\n        variable = self._add_variable_with_custom_getter(\n    File \"C:\\Users\\nxcle\\.conda\\envs\\spartan\\lib\\site-packages\\keras\\engine\\base_layer_utils.py\", line 121, in make_variable\n        init_val = functools.partial(initializer, shape, dtype=dtype)\n\n    TypeError: the first argument must be callable\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tr_data, validation_data=val_data, epochs=trainer.par.train.max_epoch, callbacks=[early_stop])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:08:23.717206600Z",
     "start_time": "2023-12-31T08:08:23.588099400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START WORKING ON  0\n",
      "comb 1 / 264\n",
      "2012\n",
      "START WORKING ON  1\n",
      "comb 2 / 264\n",
      "2013\n",
      "START WORKING ON  2\n",
      "comb 3 / 264\n",
      "2014\n",
      "START WORKING ON  3\n",
      "comb 4 / 264\n",
      "2015\n",
      "START WORKING ON  4\n",
      "comb 5 / 264\n",
      "2016\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    par = get_main_experiments(i, train_gpu=1)\n",
    "    print(par.grid.year_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T12:32:00.424190700Z",
     "start_time": "2024-01-01T12:32:00.412827900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.callbacks.EarlyStopping at 0x14a38e96950>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:25.923113300Z",
     "start_time": "2023-12-31T08:04:25.911800100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The dataset length is unknown.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 21\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mpar\u001B[38;5;241m.\u001B[39mtrain\u001B[38;5;241m.\u001B[39mapply_filter:\n\u001B[0;32m     18\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;28;01mlambda\u001B[39;00m x: trainer\u001B[38;5;241m.\u001B[39mfilter_sample_based_on_par(x))\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\spartan\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:531\u001B[0m, in \u001B[0;36mDatasetV2.__len__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    529\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe dataset is infinite.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    530\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m length\u001B[38;5;241m.\u001B[39mnumpy() \u001B[38;5;241m==\u001B[39m UNKNOWN:\n\u001B[1;32m--> 531\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe dataset length is unknown.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m length\n",
      "\u001B[1;31mTypeError\u001B[0m: The dataset length is unknown."
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "\n",
    "# Parse the dataset using the provided function\n",
    "dataset = dataset.map(trainer.parse_tfrecord)\n",
    "\n",
    "# Convert start_year and end_year to TensorFlow constants\n",
    "const_start_year = tf.constant(start_train, dtype=tf.int32)\n",
    "const_end_year = tf.constant(end_train, dtype=tf.int32)\n",
    "\n",
    "# Filter the dataset based on years\n",
    "dataset = dataset.filter(lambda x: trainer.filter_start_year(x, const_start_year))\n",
    "dataset = dataset.filter(lambda x: trainer.filter_end_year(x, const_end_year))\n",
    "\n",
    "\n",
    "# If this particular dataset must be filtered, we apply the filter at the tfrecords level.\n",
    "if trainer.par.train.apply_filter is not None:\n",
    "    if 'train' in trainer.par.train.apply_filter:\n",
    "        dataset = dataset.filter(lambda x: trainer.filter_sample_based_on_par(x))\n",
    "\n",
    "\n",
    "print(len(dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T07:13:51.842182100Z",
     "start_time": "2023-12-31T07:13:51.776516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(32, 768), dtype=float32, numpy=\narray([[ 0.06059948,  0.26095   ,  0.7160798 , ...,  0.85200804,\n         0.06284972, -0.2488295 ],\n       [-0.02092297, -1.4817846 ,  0.31832224, ..., -0.81877476,\n        -0.6297853 ,  0.424152  ],\n       [ 0.53160125, -2.2838597 ,  0.7216092 , ..., -1.3637179 ,\n        -0.06406772,  0.14400324],\n       ...,\n       [ 1.3553295 ,  0.01270566,  1.6130308 , ..., -0.28950536,\n        -0.06183032,  0.13417225],\n       [ 1.4286543 ,  0.09823425,  1.6499714 , ..., -0.28477496,\n         0.02212846,  0.13438597],\n       [ 0.9099802 , -1.1186858 ,  1.5222925 , ..., -0.05351489,\n        -0.13695595, -0.01154403]], dtype=float32)>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(trainer.train_dataset.map(lambda x, y: x).take(1)))\n",
    "sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T09:55:34.957355100Z",
     "start_time": "2024-01-01T09:55:34.093558700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([32, 768])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T09:54:54.348643700Z",
     "start_time": "2024-01-01T09:54:54.338673Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(32, 768), dtype=float32, numpy=\n array([[ 0.99465406,  0.5371273 ,  1.49615   , ...,  0.07846408,\n         -0.24202517,  0.12293728],\n        [ 0.12876956,  0.18122949,  0.16271731, ..., -0.17749994,\n         -0.53009   ,  0.6366029 ],\n        [-0.601637  , -0.45197624,  0.63373214, ..., -0.2614409 ,\n         -0.2900576 ,  0.88151765],\n        ...,\n        [ 0.71718615, -0.04619605,  1.3755744 , ...,  0.43199763,\n         -0.09834842, -0.39803368],\n        [ 1.0380542 ,  0.68655384,  1.4865812 , ...,  0.06338909,\n         -0.19234091,  0.08200923],\n        [-0.31749234, -0.7784312 ,  0.30476886, ..., -0.70451814,\n         -0.05718014,  0.83873475]], dtype=float32)>,\n <tf.Tensor: shape=(32,), dtype=int32, numpy=\n array([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n        0, 0, 1, 0, 1, 1, 0, 1, 1, 0])>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(trainer.train_dataset.take(1)))\n",
    "sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T09:28:04.468239500Z",
     "start_time": "2024-01-05T09:27:27.469102700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnz = tf.Tensor(24576.0, shape=(), dtype=float32) total_elements =  tf.Tensor(24576.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "nnz = tf.reduce_sum(tf.cast(tf.math.not_equal(dense_batch, 0), tf.float32))\n",
    "total_elements = tf.size(dense_batch, out_type=tf.dtypes.float32)\n",
    "print(\"nnz =\", nnz, \"total_elements = \", total_elements)\n",
    "sparsity = 100 * (1 - nnz / total_elements)\n",
    "print(sparsity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T05:07:13.794133500Z",
     "start_time": "2024-01-01T05:07:07.542329600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnz = tf.Tensor(24576.0, shape=(), dtype=float32) total_elements =  tf.Tensor(24576, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "nnz = tf.size(sample[0].values, out_type=tf.float32)\n",
    "total_elements = tf.reduce_prod(sample[0].dense_shape)\n",
    "print(\"nnz =\", nnz, \"total_elements = \", total_elements)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T05:13:49.552253900Z",
     "start_time": "2024-01-01T05:13:49.534780400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "768"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.input_dim\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T08:58:50.714035400Z",
     "start_time": "2024-01-01T08:58:50.706062300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # train to find which penalisaiton to use\n",
    "trainer.train_to_find_hyperparams()\n",
    "trainer.train_on_val_and_train_with_best_hyper()\n",
    "end = time.time()\n",
    "print('Ran it all in ', np.round((end - start) / 60, 5), 'min', flush=True)\n",
    "df = trainer.get_prediction_on_test_sample()\n",
    "df.to_pickle(temp_save_dir + save_name)\n",
    "\n",
    "print(df, flush=True)\n",
    "print('saved to', temp_save_dir + save_name, flush=True)\n",
    "print('We used',trainer.best_hyper_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([89, 14, 77, 57, 74, 23, 41, 47, 98, 21])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(100, 10, replace=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T01:06:08.723399200Z",
     "start_time": "2024-01-01T01:06:08.683752900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
